{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a957a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(3352:140406248606592:libvgpu.c:840)]: Initializing.....\n",
      "[HAMI-core Warn(3352:140406248606592:libvgpu.c:96)]: recursive dlsym : ompt_start_tool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib, sys, os, random, time\n",
    "import cv2, gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34afe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE编码和解码函数\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(512, 512)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == '' or pd.isna(mask_rle):\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    \n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774185a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(3352:140406248606592:libvgpu.c:859)]: Initialized\n"
     ]
    }
   ],
   "source": [
    "# 配置参数\n",
    "SEED = 42\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 1\n",
    "IMAGE_SIZE = 256  # 增加图像尺寸以获取更多细节\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "THRESHOLD = 0.5  # 二值化阈值，可以通过验证集调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae901ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc96f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的数据增强策略\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # 移除了一些可能导致问题的增强\n",
    "    A.Normalize(\n",
    "        mean=[0.625, 0.448, 0.688],\n",
    "        std=[0.131, 0.177, 0.101],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 验证集变换 - 加在此处\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(\n",
    "        mean=[0.625, 0.448, 0.688],\n",
    "        std=[0.131, 0.177, 0.101],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e6f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, mask_paths=None, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 加载图像\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 处理掩码数据\n",
    "        if self.mask_paths is not None:\n",
    "            mask_data = self.mask_paths[idx]\n",
    "            \n",
    "            # 使用已有的rle_decode函数处理RLE格式的掩码\n",
    "            if isinstance(mask_data, str):\n",
    "                mask = rle_decode(mask_data, shape=(512, 512))\n",
    "            elif isinstance(mask_data, np.ndarray):\n",
    "                mask = mask_data\n",
    "            elif isinstance(mask_data, torch.Tensor):\n",
    "                mask = mask_data.cpu().numpy()\n",
    "            else:\n",
    "                print(f\"警告：未知的掩码数据类型 - {type(mask_data)}\")\n",
    "                mask = np.zeros((512, 512), dtype=np.uint8)\n",
    "            \n",
    "            # 应用变换\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=img, mask=mask)\n",
    "                img = transformed[\"image\"]\n",
    "                mask = transformed[\"mask\"]\n",
    "            \n",
    "            # 关键修复: 始终确保mask是float类型的tensor\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask).float()\n",
    "            else:\n",
    "                # 确保tensor是float类型\n",
    "                mask = mask.float()\n",
    "            \n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "                \n",
    "            return img, mask\n",
    "        else:\n",
    "            # 只返回图像（用于测试集）\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=img)\n",
    "                img = transformed[\"image\"]\n",
    "                \n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net模型定义 - 基础模块\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2D -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733d9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        factor = 2 if bilinear else 1\n",
    "        \n",
    "        # 使用更多滤波器以增加模型容量\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085bbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的损失函数 - Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # 平滑处理以避免0/0的情况\n",
    "        intersection = (pred * target).sum(dim=(2,3))\n",
    "        union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "031b991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = 1e-7  # 添加极小值避免数值不稳定\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # 确保输入是float类型\n",
    "        pred = torch.sigmoid(pred)\n",
    "        target = target.float()  # 确保target也是float\n",
    "        \n",
    "        # 对预测值进行剪裁，确保在有效范围内\n",
    "        pred = torch.clamp(pred, self.eps, 1.0 - self.eps)\n",
    "        \n",
    "        # 二元交叉熵损失\n",
    "        bce = F.binary_cross_entropy(pred, target, reduction='none')\n",
    "        \n",
    "        # 应用focal loss公式\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_loss = (1-pt)**self.gamma * bce\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c43a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的组合损失函数\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        focal = self.focal_loss(pred, target)\n",
    "        return self.dice_weight * dice + self.focal_weight * focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1422949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"当验证集性能不再提升时提前停止训练\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 验证集性能不提升后等待多少轮停止训练\n",
    "            verbose (bool): 是否打印详细信息\n",
    "            delta (float): 性能变化的最小阈值\n",
    "            path (str): 保存检查点路径\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'早停计数: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''当验证损失减小时保存模型'''\n",
    "        if self.verbose:\n",
    "            print(f'验证损失从 ({self.val_loss_min:.6f} 降至 {val_loss:.6f})。保存模型...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d1a42b1-315a-4034-a96d-6ccba216eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_tensors(images, masks, outputs=None):\n",
    "    \"\"\"检查张量的形状和类型以及值的范围\"\"\"\n",
    "    print(f\"Images: shape={images.shape}, type={images.dtype}, device={images.device}\")\n",
    "    print(f\"Masks: shape={masks.shape}, type={masks.dtype}, device={masks.device}\")\n",
    "    \n",
    "    if outputs is not None:\n",
    "        print(f\"Outputs: shape={outputs.shape}, type={outputs.dtype}, device={outputs.device}\")\n",
    "        \n",
    "        # 检查输出值的范围\n",
    "        with torch.no_grad():\n",
    "            outputs_sigmoid = torch.sigmoid(outputs)\n",
    "            min_val = outputs_sigmoid.min().item()\n",
    "            max_val = outputs_sigmoid.max().item()\n",
    "            print(f\"输出sigmoid后的值范围: [{min_val:.6f}, {max_val:.6f}]\")\n",
    "            \n",
    "            # 检查是否有极端值\n",
    "            if min_val < 0 or max_val > 1:\n",
    "                print(\"警告: sigmoid后的输出值超出[0,1]范围!\")\n",
    "    \n",
    "    # 检查是否包含NaN或Inf\n",
    "    if torch.isnan(images).any():\n",
    "        print(\"警告: 图像包含NaN值!\")\n",
    "    if torch.isnan(masks).any():\n",
    "        print(\"警告: 掩码包含NaN值!\")\n",
    "    if outputs is not None and torch.isnan(outputs).any():\n",
    "        print(\"警告: 输出包含NaN值!\")\n",
    "    if outputs is not None and torch.isinf(outputs).any():\n",
    "        print(\"警告: 输出包含Inf值!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a082a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (images, masks) in enumerate(tqdm(dataloader)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "        \n",
    "        # 仅在特定间隔打印调试信息，比如每500个批次\n",
    "        if i % 500 == 0:\n",
    "            debug_tensors(images, masks)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 同样，限制输出频率\n",
    "        if i % 500 == 0:\n",
    "            debug_tensors(images, masks, outputs)\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # 检查损失是否有问题\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"警告: 损失值异常: {loss.item()}\")\n",
    "            continue  # 跳过这个批次\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240d0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"清理可能的内存泄漏\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 在train_with_checkpoints函数中每个epoch后调用\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e1f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证函数\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    dice_scores = []\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # 计算Dice分数\n",
    "        preds = (torch.sigmoid(outputs) > THRESHOLD).float()\n",
    "        dice = (2 * (preds * masks).sum()) / (preds.sum() + masks.sum() + 1e-8)\n",
    "        dice_scores.append(dice.item())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d321106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "@torch.no_grad()\n",
    "def predict(model, dataloader, device, threshold=THRESHOLD):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for images, filenames in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs)\n",
    "        \n",
    "        # 处理每个批次的预测\n",
    "        for pred, filename in zip(preds, filenames):\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            pred = cv2.resize(pred, (512, 512))  # 调整为原始大小\n",
    "            mask = (pred > threshold).astype(np.uint8)\n",
    "            rle = rle_encode(mask)\n",
    "            results.append([filename, rle])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "344ff872",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_with_tta(model, image, device, threshold=THRESHOLD, tta_transforms=None):\n",
    "    \"\"\"测试时增强提高预测质量\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 如果没有提供TTA变换，则使用基本变换\n",
    "    if tta_transforms is None:\n",
    "        tta_transforms = [\n",
    "            A.Compose([A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.HorizontalFlip(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.VerticalFlip(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()]),\n",
    "            A.Compose([A.Transpose(p=1.0), A.Normalize(mean=[0.625, 0.448, 0.688], std=[0.131, 0.177, 0.101]), ToTensorV2()])\n",
    "        ]\n",
    "    \n",
    "    # 应用所有变换并预测\n",
    "    preds = []\n",
    "    for transform in tta_transforms:\n",
    "        augmented = transform(image=image)\n",
    "        img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        pred = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "        \n",
    "        # 还原变换\n",
    "        if 'HorizontalFlip' in str(transform):\n",
    "            pred = np.fliplr(pred)\n",
    "        if 'VerticalFlip' in str(transform):\n",
    "            pred = np.flipud(pred)\n",
    "        if 'Transpose' in str(transform):\n",
    "            pred = np.transpose(pred)\n",
    "            \n",
    "        preds.append(pred)\n",
    "    \n",
    "    # 平均所有预测结果\n",
    "    final_pred = np.mean(preds, axis=0)\n",
    "    return (final_pred > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d963ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_checkpoints(model, train_loader, valid_loader, optimizer, \n",
    "                          criterion, scheduler, device, num_epochs, \n",
    "                          checkpoint_dir='checkpoints', accumulation_steps=4):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_dice = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # 创建日志\n",
    "    log_file = open(f\"{checkpoint_dir}/training_log.csv\", \"w\")\n",
    "    log_file.write(\"epoch,train_loss,val_loss,val_dice,learning_rate\\n\")\n",
    "    \n",
    "    # 初始化早停\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True, \n",
    "                                   path=f\"{checkpoint_dir}/early_stop_model.pth\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"第 {epoch}/{num_epochs} 轮\")\n",
    "        \n",
    "        # 训练 - 使用梯度累积\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, accumulation_steps)\n",
    "        \n",
    "        # 清理内存\n",
    "        clear_memory()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"完成第 {epoch} 轮训练，开始验证...\")\n",
    "\n",
    "        # 验证\n",
    "        val_loss, val_dice = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f\"验证完成! 损失: {val_loss:.4f}, Dice: {val_dice:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 清理内存\n",
    "        clear_memory()\n",
    "        \n",
    "        # 记录学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 写入日志\n",
    "        log_file.write(f\"{epoch},{train_loss:.4f},{val_loss:.4f},{val_dice:.4f},{current_lr:.8f}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"训练损失: {train_loss:.4f} | 验证损失: {val_loss:.4f} | Dice分数: {val_dice:.4f} | 学习率: {current_lr:.8f}\")\n",
    "        \n",
    "        # 保存检查点 - 只保存必要信息以节省空间\n",
    "        if epoch % 5 == 0 or epoch == num_epochs:  # 每5个epoch保存一次完整检查点\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_dice': val_dice,\n",
    "            }, f\"{checkpoint_dir}/checkpoint_epoch_{epoch}.pth\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_dice > best_dice:\n",
    "            print(f\"Dice分数从 {best_dice:.4f} 提高到 {val_dice:.4f}. 正在保存模型...\")\n",
    "            best_dice = val_dice\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pth\")\n",
    "        \n",
    "        # 检查早停条件\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"触发早停! 训练停止。\")\n",
    "            break\n",
    "    \n",
    "    log_file.close()\n",
    "    print(f\"训练完成! 最佳Dice分数: {best_dice:.4f} (第{best_epoch}轮)\")\n",
    "    return best_dice, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b5fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 设置随机种子\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    # 加载数据\n",
    "    try:\n",
    "        print(\"正在加载训练数据...\")\n",
    "        train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "        train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据时出错: {e}\")\n",
    "        print(\"请确保'数据集/train_mask.csv'文件存在并且格式正确!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"已加载 {len(train_mask)} 条训练数据\")\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    train_idx, valid_idx = [], []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    \n",
    "    train_df = train_mask.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"训练集: {len(train_df)} 样本, 验证集: {len(valid_df)} 样本\")\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_ds = BuildingSegmentationDataset(\n",
    "        train_df['name'].values,\n",
    "        train_df['mask'].fillna('').values,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    valid_ds = BuildingSegmentationDataset(\n",
    "        valid_df['name'].values,\n",
    "        valid_df['mask'].fillna('').values,\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = D.DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = D.DataLoader(\n",
    "        valid_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 打印模型摘要\n",
    "    print(f\"模型已创建并加载到设备: {DEVICE}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"模型总参数量: {total_params:,}\")\n",
    "    \n",
    "    # 优化器和学习率调度\n",
    "    # 在main函数中替换优化器定义\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4, eps=1e-8)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 损失函数\n",
    "    criterion = CombinedLoss(dice_weight=0.7, focal_weight=0.3)\n",
    "    \n",
    "    # 使用改进的训练循环\n",
    "    best_dice, best_epoch = train_with_checkpoints(\n",
    "        model, train_loader, valid_loader, optimizer, \n",
    "        criterion, scheduler, DEVICE, EPOCHS, checkpoint_dir='model_checkpoints'\n",
    "    )\n",
    "    \n",
    "    print(f\"训练完成! 最佳Dice分数: {best_dice:.4f} 在第{best_epoch}轮\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7673b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set():\n",
    "    # 加载最佳模型进行预测\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('model_checkpoints/best_model.pth'))\n",
    "    except:\n",
    "        model.load_state_dict(torch.load('best_building_segmentation_model.pth'))\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # 创建测试数据集\n",
    "    test_paths = []\n",
    "    # 确保这个路径指向您的测试图像目录\n",
    "    for file in os.listdir('数据集/test_a'):\n",
    "        if file.endswith('.jpg') or file.endswith('.tif'):\n",
    "            test_paths.append(os.path.join('数据集/test_a', file))\n",
    "    \n",
    "    # 使用更小的批次\n",
    "    test_batch_size = 1  # 单张预测避免内存问题\n",
    "    \n",
    "    test_ds = BuildingSegmentationDataset(\n",
    "        test_paths,\n",
    "        None,  # 测试集没有掩码数据\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = D.DataLoader(\n",
    "        test_ds, batch_size=test_batch_size, shuffle=False,\n",
    "        num_workers=1, pin_memory=True  # 使用更少的worker和更小的批量\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    test_files = [os.path.basename(p) for p in test_paths]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for images in tqdm(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            \n",
    "            # 处理每个批次的预测\n",
    "            for pred in preds:\n",
    "                if i >= len(test_files):  # 安全检查\n",
    "                    break\n",
    "                    \n",
    "                pred = pred.cpu().numpy().squeeze()\n",
    "                # 及时清理GPU内存\n",
    "                clear_memory()\n",
    "                \n",
    "                pred = cv2.resize(pred, (512, 512))  # 调整为原始大小\n",
    "                mask = (pred > THRESHOLD).astype(np.uint8)\n",
    "                rle = rle_encode(mask)\n",
    "                results.append([test_files[i], rle])\n",
    "                i += 1\n",
    "    \n",
    "    submission = pd.DataFrame(results, columns=['name', 'mask'])\n",
    "    submission.to_csv('submission.csv', index=False, header=False, sep='\\t')\n",
    "    print(\"预测完成! 结果已保存到 submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
