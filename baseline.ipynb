{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7a957a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib, sys, os, random, time\n",
    "import cv2, gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torchvision\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b34afe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE编码和解码函数\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(512, 512)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == '' or pd.isna(mask_rle):\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    \n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "774185a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "SEED = 42\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 384  # 增加图像尺寸以获取更多细节\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "THRESHOLD = 0.5  # 二值化阈值，可以通过验证集调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae901ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc96f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的数据增强策略\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.GaussNoise(p=0.3),\n",
    "    A.Normalize(\n",
    "        mean=[0.625, 0.448, 0.688],\n",
    "        std=[0.131, 0.177, 0.101],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(\n",
    "        mean=[0.625, 0.448, 0.688],\n",
    "        std=[0.131, 0.177, 0.101],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3e6f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, mask_paths=None, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths  # 这里可能存储的是掩码数据而不是路径\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 加载图像\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 处理掩码数据\n",
    "        if self.mask_paths is not None:\n",
    "            # 检查 mask_paths 是否是数据而不是路径\n",
    "            mask_data = self.mask_paths[idx]\n",
    "            \n",
    "            # 如果 mask_data 是字符串，且看起来是空格分隔的数字序列\n",
    "            if isinstance(mask_data, str) and ' ' in mask_data:\n",
    "                try:\n",
    "                    # 将空格分隔的数字转换为数组\n",
    "                    mask_values = list(map(int, mask_data.split()))\n",
    "                    \n",
    "                    # 假设这是一个扁平化的二值掩码，将其重构为方形矩阵\n",
    "                    # 估计尺寸（取近似平方根）\n",
    "                    size = int(np.sqrt(len(mask_values) / 2)) * 2  # 确保是偶数\n",
    "                    \n",
    "                    # 将扁平数组转为掩码矩阵（如果长度不匹配则填充0）\n",
    "                    if len(mask_values) >= size * size:\n",
    "                        mask = np.array(mask_values[:size*size]).reshape(size, size)\n",
    "                    else:\n",
    "                        # 填充\n",
    "                        padded = mask_values + [0] * (size * size - len(mask_values))\n",
    "                        mask = np.array(padded).reshape(size, size)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # 如果转换失败，创建空掩码\n",
    "                    print(f\"警告：掩码转换失败 - {str(e)}\")\n",
    "                    mask = np.zeros((384, 384), dtype=np.float32)\n",
    "            \n",
    "            # 如果是路径，尝试加载文件（但很可能不是这种情况）\n",
    "            elif isinstance(mask_data, str) and (mask_data.endswith('.npy') or \n",
    "                                              mask_data.endswith('.png') or \n",
    "                                              mask_data.endswith('.jpg')):\n",
    "                try:\n",
    "                    if mask_data.endswith('.npy'):\n",
    "                        mask = np.load(mask_data)\n",
    "                    else:\n",
    "                        mask = cv2.imread(mask_data, cv2.IMREAD_GRAYSCALE)\n",
    "                except Exception as e:\n",
    "                    print(f\"警告：掩码文件加载失败 - {str(e)}\")\n",
    "                    mask = np.zeros((384, 384), dtype=np.float32)\n",
    "            \n",
    "            # 如果是NumPy数组，直接使用\n",
    "            elif isinstance(mask_data, np.ndarray):\n",
    "                mask = mask_data\n",
    "            \n",
    "            # 如果是torch.Tensor，转为NumPy\n",
    "            elif isinstance(mask_data, torch.Tensor):\n",
    "                mask = mask_data.cpu().numpy()\n",
    "            \n",
    "            # 其他情况，创建空掩码\n",
    "            else:\n",
    "                print(f\"警告：未知的掩码数据类型 - {type(mask_data)}\")\n",
    "                mask = np.zeros((384, 384), dtype=np.float32)\n",
    "            \n",
    "            # 应用变换（如果有）\n",
    "            if self.transform is not None:\n",
    "                # 如果使用 albumentations\n",
    "                try:\n",
    "                    transformed = self.transform(image=img, mask=mask)\n",
    "                    img = transformed['image']\n",
    "                    mask = transformed['mask']\n",
    "                # 如果使用 torchvision.transforms\n",
    "                except:\n",
    "                    img = self.transform(img)\n",
    "                    mask = cv2.resize(mask, (384, 384), interpolation=cv2.INTER_NEAREST)\n",
    "            else:\n",
    "                # 确保mask格式正确\n",
    "                if not isinstance(mask, np.ndarray):\n",
    "                    mask = np.array(mask)\n",
    "                \n",
    "                # 调整尺寸\n",
    "                mask = cv2.resize(mask, (384, 384), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # 转换为tensor\n",
    "            if not isinstance(img, torch.Tensor):\n",
    "                img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
    "            \n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask).float()\n",
    "                \n",
    "            # 确保mask有通道维度\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "                \n",
    "            return img, mask\n",
    "        else:\n",
    "            # 只返回图像\n",
    "            if self.transform is not None:\n",
    "                try:  # albumentations\n",
    "                    transformed = self.transform(image=img)\n",
    "                    img = transformed['image']\n",
    "                except:  # torchvision\n",
    "                    img = self.transform(img)\n",
    "            else:\n",
    "                img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
    "                \n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54a092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net模型定义 - 基础模块\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2D -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "733d9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "299417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        factor = 2 if bilinear else 1\n",
    "        \n",
    "        # 使用更多滤波器以增加模型容量\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "085bbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的损失函数 - Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # 平滑处理以避免0/0的情况\n",
    "        intersection = (pred * target).sum(dim=(2,3))\n",
    "        union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "031b991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的损失函数 - Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # 二元交叉熵损失\n",
    "        bce = F.binary_cross_entropy(pred, target, reduction='none')\n",
    "        \n",
    "        # 应用focal loss公式\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_loss = (1-pt)**self.gamma * bce\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0c43a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的组合损失函数\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        focal = self.focal_loss(pred, target)\n",
    "        return self.dice_weight * dice + self.focal_weight * focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a082a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e1f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证函数\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    dice_scores = []\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # 计算Dice分数\n",
    "        preds = (torch.sigmoid(outputs) > THRESHOLD).float()\n",
    "        dice = (2 * (preds * masks).sum()) / (preds.sum() + masks.sum() + 1e-8)\n",
    "        dice_scores.append(dice.item())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d321106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "@torch.no_grad()\n",
    "def predict(model, dataloader, device, threshold=THRESHOLD):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for images, filenames in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs)\n",
    "        \n",
    "        # 处理每个批次的预测\n",
    "        for pred, filename in zip(preds, filenames):\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            pred = cv2.resize(pred, (512, 512))  # 调整为原始大小\n",
    "            mask = (pred > threshold).astype(np.uint8)\n",
    "            rle = rle_encode(mask)\n",
    "            results.append([filename, rle])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4b5fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主训练循环\n",
    "def main():\n",
    "    # 加载数据\n",
    "    # 确保这个路径正确指向您的train_mask.csv文件位置\n",
    "    train_mask = pd.read_csv('数据集/train_mask.csv', sep='\\t', names=['name', 'mask'])\n",
    "    # 确保这个路径前缀与您的训练图像目录一致\n",
    "    train_mask['name'] = train_mask['name'].apply(lambda x: '数据集/train/' + x)\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    train_idx, valid_idx = [], []\n",
    "    for i in range(len(train_mask)):\n",
    "        if i % 7 == 0:\n",
    "            valid_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    \n",
    "    train_df = train_mask.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train_mask.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_ds = BuildingSegmentationDataset(\n",
    "        train_df['name'].values,\n",
    "        train_df['mask'].fillna('').values,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    valid_ds = BuildingSegmentationDataset(\n",
    "        valid_df['name'].values,\n",
    "        valid_df['mask'].fillna('').values,\n",
    "        transform=valid_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = D.DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = D.DataLoader(\n",
    "        valid_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 优化器和学习率调度\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 损失函数\n",
    "    criterion = CombinedLoss(dice_weight=0.7, focal_weight=0.3)\n",
    "    \n",
    "    # 训练循环\n",
    "    best_dice = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_dice = validate(model, valid_loader, criterion, DEVICE)\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_dice > best_dice:\n",
    "            print(f\"Dice improved from {best_dice:.4f} to {val_dice:.4f}. Saving model...\")\n",
    "            best_dice = val_dice\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_building_segmentation_model.pth')\n",
    "    \n",
    "    print(f\"Training complete! Best Dice: {best_dice:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7673b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set():\n",
    "    # 加载最佳模型进行预测\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model.load_state_dict(torch.load('best_building_segmentation_model.pth'))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 创建测试数据集\n",
    "    test_paths = []\n",
    "    # 确保这个路径指向您的测试图像目录\n",
    "    for file in os.listdir('数据集/test_a'):\n",
    "        if file.endswith('.jpg') or file.endswith('.tif'):\n",
    "            test_paths.append(os.path.join('数据集/test_a', file))\n",
    "    \n",
    "    test_ds = BuildingSegmentationDataset(\n",
    "        test_paths,\n",
    "        [\"\"] * len(test_paths),\n",
    "        transform=valid_transform,\n",
    "        test_mode=True\n",
    "    )\n",
    "    \n",
    "    test_loader = D.DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 预测并保存结果\n",
    "    results = predict(model, test_loader, DEVICE)\n",
    "    submission = pd.DataFrame(results, columns=['name', 'mask'])\n",
    "    submission.to_csv('submission.csv', index=False, header=False, sep='\\t')\n",
    "    print(\"Prediction complete! Results saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afb485e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2905c58cc8a492e946e9a6bc6e1d204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：未知的掩码数据类型 - <class 'str'>\n",
      "警告：未知的掩码数据类型 - <class 'str'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_10028/2992724474.py\", line 73, in __getitem__\n    transformed = self.transform(image=img, mask=mask)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 493, in __call__\n    self.preprocess(data)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 527, in preprocess\n    self._check_shape_consistency(shapes, volume_shapes)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 778, in _check_shape_consistency\n    self._check_shapes(shapes, self.is_check_shapes)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 705, in _check_shapes\n    raise ValueError(\nValueError: Height and Width of image, mask or masks should be equal. You can disable shapes check by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure about your data consistency).\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_10028/2992724474.py\", line 78, in __getitem__\n    img = self.transform(img)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 479, in __call__\n    raise KeyError(msg)\nKeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     predict_test_set()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[32m     67\u001b[39m val_loss, val_dice = validate(model, valid_loader, criterion, DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device)\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      4\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mKeyError\u001b[39m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_10028/2992724474.py\", line 73, in __getitem__\n    transformed = self.transform(image=img, mask=mask)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 493, in __call__\n    self.preprocess(data)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 527, in preprocess\n    self._check_shape_consistency(shapes, volume_shapes)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 778, in _check_shape_consistency\n    self._check_shapes(shapes, self.is_check_shapes)\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 705, in _check_shapes\n    raise ValueError(\nValueError: Height and Width of image, mask or masks should be equal. You can disable shapes check by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure about your data consistency).\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_10028/2992724474.py\", line 78, in __getitem__\n    img = self.transform(img)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/albumentations/core/composition.py\", line 479, in __call__\n    raise KeyError(msg)\nKeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：未知的掩码数据类型 - <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    predict_test_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
